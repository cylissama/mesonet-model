{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77fc0f30-6de5-4ff0-bb78-77ddf7a8ad70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @BOSS_JAEHYUN: Jaehyun and Jaeminâ€™s high five ðŸ– https://t.co/D9XnbQQJEq\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(r\"/Users/cylis/MachineLearning/ml_course/June_Labeling/Data/2022-01-01/581tweet.txt\",'r') as file_in:\n",
    "    for line in file_in:\n",
    "        tweet = json.loads(line)\n",
    "        text = tweet['text']\n",
    "        print(text)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62565941-3bdd-4213-ac02-5b4192cc5cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text','w',encoding='utf-8') as file_out:\n",
    "    file_out.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a0e171d-d370-4723-b9ba-997c73f29e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "url_pattern = re.compile(r'http\\S+')\n",
    "hashtag_pattern = re.compile(r'#\\S+')\n",
    "mention_pattern = re.compile(r'@\\S+')\n",
    "rule = re.compile(r'[^a-zA-Z0-9\\s]')\n",
    "# rule = re.compile(r'[^a-zA-Z\\s]')\n",
    "rule_all = re.compile(r'[^a-zA-Z0-9\\s]')\n",
    "\n",
    "\n",
    "class User():\n",
    "    '''\n",
    "    TwitterUser class can used to save a user\n",
    "    Including userbased features\n",
    "    '''\n",
    "    #user_feature = []\n",
    "    def __init__(self, tweet_json):\n",
    "        self.user_json = tweet_json['user']\n",
    "        self.id_str = self.user_json['id_str']\n",
    "        self.screen_name = self.user_json['screen_name']\n",
    "        self.name = self.user_json['name']\n",
    "        self.created_at = self.json_date_to_stamp(self.user_json['created_at'])\n",
    "        self.followers_count = self.get_followers_count()\n",
    "        self.friends_count = self.get_friends_count()\n",
    "        self.statuses_count = self.get_statuses_count()\n",
    "\n",
    "    def json_date_to_stamp(self, json_date):\n",
    "        '''\n",
    "        exchange date from json format to timestamp(int)\n",
    "        input:\n",
    "            date from json\n",
    "        output:\n",
    "            int\n",
    "        '''\n",
    "        time_strpt = time.strptime(json_date, '%a %b %d %H:%M:%S +0000 %Y')\n",
    "        stamp = int(time.mktime(time_strpt))\n",
    "        return stamp\n",
    "\n",
    "    def json_date_to_os(self, json_date):\n",
    "        '''\n",
    "        exchange date from json format to linux OS format\n",
    "        input:\n",
    "            date from json\n",
    "        output:\n",
    "            datetime\n",
    "        '''\n",
    "        time_strpt = time.strftime('%Y-%m-%d %H:%M:%S',\n",
    "                                   time.strptime(json_date, '%a %b %d %H:%M:%S +0000 %Y'))\n",
    "        os_time = datetime.strptime(str(time_strpt), '%Y-%m-%d %H:%M:%S')\n",
    "        return os_time\n",
    "    \n",
    "    def get_screen_name(self):\n",
    "        '''\n",
    "        return user screen name\n",
    "        '''\n",
    "        return self.screen_name\n",
    "\n",
    "    def get_followers_count(self):\n",
    "        '''\n",
    "        return follower count\n",
    "        '''\n",
    "        followers_count = self.user_json['followers_count']\n",
    "        if followers_count == 0:\n",
    "            followers_count = 1\n",
    "        return followers_count\n",
    "\n",
    "    def get_friends_count(self):\n",
    "        '''\n",
    "        return friends count\n",
    "        '''\n",
    "        friends_count = self.user_json['friends_count']\n",
    "        if friends_count == 0:\n",
    "            friends_count = 1\n",
    "        return friends_count\n",
    "\n",
    "    def get_statuses_count(self):\n",
    "        '''\n",
    "        return statuses count\n",
    "        '''\n",
    "        statuses_count = self.user_json['statuses_count']\n",
    "        if statuses_count == 0:\n",
    "            statuses_count = 1\n",
    "        return statuses_count\n",
    "\n",
    "    def get_user_age(self):\n",
    "        '''\n",
    "        Age of an account\n",
    "        get age feature of an account, remember call this function all the time. Time exchange\n",
    "        '''\n",
    "        account_start_time = self.json_date_to_os(self.user_json['created_at'])\n",
    "        now_time = datetime.now()\n",
    "        account_age = (now_time-account_start_time).days\n",
    "        if account_age == 0:\n",
    "            account_age = 1\n",
    "        return account_age\n",
    "\n",
    "    def get_user_favourites(self):\n",
    "        '''\n",
    "        get user favourites count\n",
    "        '''\n",
    "        favourites_count = self.user_json['favourites_count']\n",
    "        return favourites_count\n",
    "\n",
    "    def get_user_lists(self):\n",
    "        '''\n",
    "        get user lists\n",
    "        '''\n",
    "        listed_count = self.user_json['listed_count']\n",
    "        return listed_count\n",
    "    \n",
    "    def get_description_len(self):\n",
    "        desc_text = self.user_json['description']\n",
    "        # filter_text = self.text_filter(desc_text)\n",
    "        if desc_text is None:\n",
    "            return 0\n",
    "        else:\n",
    "            return len(desc_text)\n",
    "\n",
    "    def text_filter(self, text):\n",
    "        # filter email\n",
    "        new_text = re.sub(url_pattern,' ',text)\n",
    "\n",
    "        # filter hashtag\n",
    "        #new_text = re.sub(hashtag_pattern,'',new_text)\n",
    "\n",
    "        # filter mention\n",
    "        new_text = re.sub(mention_pattern,' ',new_text)\n",
    "\n",
    "        # . _ - link words together\n",
    "        new_text = re.sub('\\.|_|-', '', new_text)\n",
    "\n",
    "        # keep\n",
    "        new_text = re.sub(rule_all, ' ',new_text)\n",
    "        # new_text = re.sub(rule, ' ',new_text)\n",
    "\n",
    "        # filter \\t \\n\n",
    "        new_text = re.sub('\\t|\\n|\\r|\\v',' ', new_text)\n",
    "        # filter multi spaces\n",
    "        new_text = re.sub(' +', ' ', new_text)\n",
    "\n",
    "        # remove space of head or tail\n",
    "        new_text = re.sub('^ | $','', new_text)\n",
    "\n",
    "        return new_text\n",
    "\n",
    "    def get_ratio_follow_friend(self):\n",
    "        return self.get_followers_count()/self.get_friends_count()\n",
    "\n",
    "\n",
    "class Tweet():\n",
    "    '''\n",
    "    TwitterTweet class can used to save a tweet\n",
    "    '''\n",
    "    def __init__(self, tweet_json):\n",
    "        self.tweet_json = tweet_json\n",
    "        self.user = User(tweet_json)\n",
    "\n",
    "\n",
    "        self.text = tweet_json['text']\n",
    "        self.timestr = self.json_date_to_stamp(tweet_json['created_at'])\n",
    "        self.tid_str = self.tweet_json['id_str']\n",
    "\n",
    "    def json_date_to_stamp(self, json_date):\n",
    "        '''\n",
    "        exchange date from json format to timestamp(int)\n",
    "        input:\n",
    "            date from json\n",
    "        output:\n",
    "            int\n",
    "        '''\n",
    "        time_strpt = time.strptime(json_date, '%a %b %d %H:%M:%S +0000 %Y')\n",
    "        stamp = int(time.mktime(time_strpt))\n",
    "        return stamp\n",
    "    \n",
    "    def get_id(self):\n",
    "        return self.tid_str\n",
    "\n",
    "    def is_en(self):\n",
    "        return self.tweet_json[\"lang\"]==\"en\"\n",
    "\n",
    "    def get_create_at(self):\n",
    "        '''\n",
    "        get create at\n",
    "        '''\n",
    "        return self.timestr\n",
    "\n",
    "    def get_retweet_count(self):\n",
    "        '''\n",
    "        get retweet count\n",
    "        '''\n",
    "        retweet_count = self.tweet_json['retweet_count']\n",
    "        return retweet_count\n",
    "\n",
    "    def get_hashtag_count(self):\n",
    "        '''\n",
    "        get number of hashtags\n",
    "        '''\n",
    "        hashtags = self.tweet_json['entities']['hashtags']\n",
    "        return len(hashtags)\n",
    "\n",
    "    def get_mention_count(self):\n",
    "        '''\n",
    "        get number of hashtags\n",
    "        '''\n",
    "        mentions = self.tweet_json['entities']['user_mentions']\n",
    "        return len(mentions)\n",
    "\n",
    "    def get_url_count(self):\n",
    "        '''\n",
    "        get number of urls\n",
    "        '''\n",
    "        urls = self.tweet_json['entities']['urls']\n",
    "        return len(urls)\n",
    "\n",
    "    def get_text_len(self):\n",
    "        '''\n",
    "        return chars of text\n",
    "        '''\n",
    "        return len(self.text)\n",
    "\n",
    "    def get_text_digits(self):\n",
    "        '''\n",
    "        return number of digits in tweet\n",
    "        '''\n",
    "        count_digits = 0\n",
    "        for ch in self.text:\n",
    "            if ord(ch)>=48 and ord(ch)<=57:\n",
    "                count_digits = count_digits+1\n",
    "        return count_digits\n",
    "\n",
    "    def get_tweet_features(self):\n",
    "        feature_list = []\n",
    "        feature_list.append(self.user.get_user_age())           # 1\n",
    "        feature_list.append(self.user.get_description_len())    # 2\n",
    "        feature_list.append(self.user.get_followers_count())    # 3\n",
    "        feature_list.append(self.user.get_friends_count())      # 4\n",
    "        feature_list.append(self.user.get_user_favourites())    # 5\n",
    "        feature_list.append(self.user.get_user_lists())         # 6\n",
    "        feature_list.append(self.user.get_statuses_count())     # 7\n",
    "        feature_list.append(self.get_hashtag_count())           # 8\n",
    "        feature_list.append(self.get_mention_count())           # 9\n",
    "        feature_list.append(self.get_url_count())               # 10\n",
    "        feature_list.append(self.get_text_len())                # 11\n",
    "        feature_list.append(self.get_text_digits())             # 12\n",
    "\n",
    "        return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9ef5741-3fb7-433e-94a2-3fdc3133b0e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'context_locals'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtool\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mTwitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tweet\n\u001b[1;32m      6\u001b[0m start_date \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m20201117\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m end_date \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m20210521\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml_course/lib/python3.11/site-packages/tool/__init__.py:11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#  Copyright (c) 2009â€”2010 Andrey Mikhailenko and contributors\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#  Software Foundation. See the file README for copying conditions.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcontext_locals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m app, local\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapplication\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Application, WebApplication \u001b[38;5;66;03m#, current_app as app\u001b[39;00m\n\u001b[1;32m     15\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mApplication\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWebApplication\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'context_locals'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "from tool.Twitter import Tweet\n",
    "\n",
    "start_date = \"20201117\"\n",
    "end_date = \"20210521\"\n",
    "start_date_datetime = datetime.datetime.strptime(start_date, '%Y%m%d')\n",
    "end_date_datetime = datetime.datetime.strptime(end_date, '%Y%m%d')\n",
    "proc_date = start_date_datetime\n",
    "duration = 300   # t\n",
    "\n",
    "data_check_list = os.listdir(\"Data/\")\n",
    "data_check_dic = {i:1 for i in data_check_list}\n",
    "\n",
    "for _ in range(duration):\n",
    "    # process the data in this date\n",
    "    proc_date_str = proc_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    input_data_folder_path = \"Data/\"+proc_date_str+\"/\"\n",
    "    output_data_folder_path =   \"Tmp/\"+proc_date_str+\"/\"\n",
    "\n",
    "\n",
    "    if not proc_date_str in data_check_dic.keys():\n",
    "        proc_date = proc_date+datetime.timedelta(days=1)\n",
    "        if proc_date == end_date_datetime:\n",
    "            break\n",
    "        continue\n",
    "    \n",
    "    if not os.path.exists(output_data_folder_path):\n",
    "        os.makedirs(output_data_folder_path)\n",
    "\n",
    "    output_data_file = output_data_folder_path+\"tweet_feature\"\n",
    "    with open(output_data_file, 'w', encoding='utf-8') as file_out:\n",
    "\n",
    "\n",
    "        for filename in os.listdir(input_data_folder_path):\n",
    "            input_data_path = input_data_folder_path+filename\n",
    "\n",
    "            with open(input_data_path, 'r', encoding='utf-8', errors='ignore') as file_in:\n",
    "                \n",
    "                for line in file_in:\n",
    "                    try:\n",
    "                        tweet = json.loads(line)\n",
    "                        tweet_obj = Tweet(tweet)\n",
    "                    except:\n",
    "                        print(tweet)\n",
    "                        continue\n",
    "                    \n",
    "                    if not tweet_obj.is_en():\n",
    "                        continue\n",
    "\n",
    "                    tweet_id = tweet_obj.get_id()\n",
    "                    user_id = tweet_obj.user.id_str\n",
    "                    feature_list = tweet_obj.get_tweet_features()\n",
    "                    file_out.write(tweet_id+'\\t'+user_id+'\\t')\n",
    "\n",
    "                    for f in feature_list:\n",
    "                        file_out.write(str(f))\n",
    "                        file_out.write('\\t')\n",
    "                    file_out.write('\\n')\n",
    "                    file_out.flush()\n",
    "                    \n",
    "    proc_date = proc_date+datetime.timedelta(days=1)\n",
    "    if proc_date == end_date_datetime:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8af977f6-3547-4cd0-aae9-15144ff6f33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-01\n",
      "Shingling tweets...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Tmp/2022-01-01/tweet_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 67\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# if not os.path.exists(output_data_folder_path):\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m#     os.makedirs(output_data_folder_path)\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# tweet_limit = 10000\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_data_file \u001b[38;5;129;01min\u001b[39;00m input_data_file_list:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(input_data_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file_in:\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file_in:\n\u001b[1;32m     69\u001b[0m             \u001b[38;5;66;03m# tweet_limit -= 1\u001b[39;00m\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;66;03m# if tweet_limit < 0:\u001b[39;00m\n\u001b[1;32m     71\u001b[0m             \u001b[38;5;66;03m#     break\u001b[39;00m\n\u001b[1;32m     73\u001b[0m             tweet_id \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml_course/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Tmp/2022-01-01/tweet_text'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import datetime\n",
    "import binascii\n",
    "\n",
    "start_date = \"20220101\"\n",
    "end_date = \"20220102\"\n",
    "start_date_datetime = datetime.datetime.strptime(start_date, '%Y%m%d')\n",
    "end_date_datetime = datetime.datetime.strptime(end_date, '%Y%m%d')\n",
    "proc_date = start_date_datetime\n",
    "duration = 300   #\n",
    "future_days = 5\n",
    "\n",
    "data_check_list = os.listdir(\"Data/\")\n",
    "data_check_dic = {i:1 for i in data_check_list}\n",
    "\n",
    "for _ in range(duration):\n",
    "    # process the data in this date\n",
    "    proc_date_str = proc_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    print(proc_date_str)\n",
    "\n",
    "    input_data_folder_path = \"Tmp/\"+proc_date_str+\"/\"\n",
    "    output_data_folder_path =   \"Tmp/\"+proc_date_str+\"/\"\n",
    "\n",
    "    if not proc_date_str in data_check_dic.keys():\n",
    "        proc_date = proc_date+datetime.timedelta(days=1)\n",
    "        if proc_date == end_date_datetime:\n",
    "            break\n",
    "        continue\n",
    "\n",
    "    input_data_file_list = []\n",
    "    for i in range(future_days):\n",
    "        future_date =  proc_date+datetime.timedelta(days=i)\n",
    "        future_date_str = future_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        if not future_date_str in data_check_dic.keys():\n",
    "            continue\n",
    "        future_data_folder_path = \"Tmp/\"+future_date_str+\"/\"\n",
    "        future_data_file = future_data_folder_path+\"tweet_text\"\n",
    "        input_data_file_list.append(future_data_file)\n",
    "\n",
    "\n",
    "    numtweets = 0\n",
    "    numHashes = 10\n",
    "    curtweets = 0\n",
    "\n",
    "    print('Shingling tweets...')\n",
    "\n",
    "    # The current shingle ID value to assign to the next new shingle we \n",
    "    # encounter. When a shingle gets added to the dictionary, we'll increment this\n",
    "    # value.\n",
    "    curShingleID = 0\n",
    "\n",
    "    # Create a dictionary of the articles, mapping the article identifier (e.g., \n",
    "    # \"t8470\") to the list of shingle IDs that appear in the document.\n",
    "    tweetAsShingleSets = {}\n",
    "    tweetNames = []\n",
    "    totalShingles = 0\n",
    "\n",
    "    # if not os.path.exists(output_data_folder_path):\n",
    "    #     os.makedirs(output_data_folder_path)\n",
    "\n",
    "    # tweet_limit = 10000\n",
    "    for input_data_file in input_data_file_list:\n",
    "        with open(input_data_file, 'r', encoding='utf-8', errors='ignore') as file_in:\n",
    "            for line in file_in:\n",
    "                # tweet_limit -= 1\n",
    "                # if tweet_limit < 0:\n",
    "                #     break\n",
    "                \n",
    "                tweet_id = line.strip().split('\\t')[0]\n",
    "                try:\n",
    "                    tweet_text =  line.strip().split('\\t')[2]\n",
    "                except:\n",
    "                    continue\n",
    "                tweet_token = tweet_text.strip().split()\n",
    "                \n",
    "                # Maintain a list of all tweet IDs.\n",
    "                tweetNames.append(tweet_id)\n",
    "\n",
    "                curtweets = curtweets + 1\n",
    "\n",
    "                # 'shinglesInDoc' will hold all of the unique shingle IDs present in the \n",
    "                # current document. If a shingle ID occurs multiple times in the document,\n",
    "                # it will only appear once in the set (this is a property of Python sets).\n",
    "                shinglesInTweet = set()\n",
    "\n",
    "                # For each word in the document...\n",
    "                for index in range(0, len(tweet_token) - 2):\n",
    "                    # Construct the shingle text by combining three words together.\n",
    "                    shingle =tweet_token[index] + ' ' + tweet_token[index + 1] + ' ' + tweet_token[index + 2]\n",
    "                    # shingle = tweet_text[index] + ' ' + tweet_text[index + 1] + ' ' + tweet_text[index + 2]\n",
    "    \n",
    "\n",
    "                    # Hash the shingle to a 32-bit integer.\n",
    "                    crc = binascii.crc32(bytes(shingle, encoding=\"ascii\")) & 0xffffffff\n",
    "\n",
    "            \n",
    "                    # Add the hash value to the list of shingles for the current document. \n",
    "                    # Note that set objects will only add the value to the set if the set \n",
    "                    # doesn't already contain it. \n",
    "                    shinglesInTweet.add(crc)\n",
    "\n",
    "                # Store the completed list of shingles for this document in the dictionary.\n",
    "                tweetAsShingleSets[tweet_id] = shinglesInTweet\n",
    "    numtweets = curtweets\n",
    "\n",
    "\n",
    "    # =============================================================================\n",
    "    #                     Define Triangle Matrices\n",
    "    # =============================================================================\n",
    "\n",
    "    # Define virtual Triangle matrices to hold the similarity values. For storing\n",
    "    # similarities between pairs, we only need roughly half the elements of a full\n",
    "    # matrix. Using a triangle matrix requires less than half the memory of a full\n",
    "    # matrix, and can protect the programmer from inadvertently accessing one of\n",
    "    # the empty/invalid cells of a full matrix.\n",
    "\n",
    "    # Calculate the number of elements needed in our triangle matrix\n",
    "    numElems = int(numtweets * (numtweets - 1) / 2)\n",
    "\n",
    "    # Initialize two empty lists to store the similarity values. \n",
    "    # 'JSim' will be for the actual Jaccard Similarity values. \n",
    "    # 'estJSim' will be for the estimated Jaccard Similarities found by comparing\n",
    "    # the MinHash signatures.\n",
    "    JSim = [0 for x in range(numElems)]\n",
    "    estJSim = [0 for x in range(numElems)]\n",
    "\n",
    "    def getTriangleIndex(i, j):\n",
    "    # If i == j that's an error.\n",
    "        if i == j:\n",
    "            sys.stderr.write(\"Can't access triangle matrix with i == j\")\n",
    "            sys.exit(1)\n",
    "        # If j < i just swap the values.\n",
    "        if j < i:\n",
    "            i, j = j, i\n",
    "            # temp = i\n",
    "            # i = j\n",
    "            # j = temp\n",
    "\n",
    "        # Calculate the index within the triangular array.\n",
    "        # This fancy indexing scheme is taken from pg. 211 of:\n",
    "        # http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n",
    "        # But I adapted it for a 0-based index.\n",
    "        # Note: The division by two should not truncate, it\n",
    "        #       needs to be a float. \n",
    "        k = int(i * (numtweets - (i + 1) / 2.0) + j - i) - 1\n",
    "\n",
    "        return k\n",
    "\n",
    "    # =============================================================================\n",
    "    #                 Generate MinHash Signatures\n",
    "    # =============================================================================\n",
    "    print ('Generating random hash functions...')\n",
    "\n",
    "    # Record the maximum shingle ID that we assigned.\n",
    "    maxShingleID = 2**32-1\n",
    "\n",
    "    # We need the next largest prime number above 'maxShingleID'.\n",
    "    # I looked this value up here: \n",
    "    # http://compoasso.free.fr/primelistweb/page/prime/liste_online_en.php\n",
    "    nextPrime = 4294967311\n",
    "\n",
    "\n",
    "    # Our random hash function will take the form of:\n",
    "    #   h(x) = (a*x + b) % c\n",
    "    # Where 'x' is the input value, 'a' and 'b' are random coefficients, and 'c' is\n",
    "    # a prime number just greater than maxShingleID.\n",
    "\n",
    "    # Generate a list of 'k' random coefficients for the random hash functions,\n",
    "    # while ensuring that the same value does not appear multiple times in the \n",
    "    # list.\n",
    "    def pickRandomCoeffs(k):\n",
    "        # Create a list of 'k' random values.\n",
    "        randList = []\n",
    "    \n",
    "        while k > 0:\n",
    "            # Get a random shingle ID.\n",
    "            randIndex = random.randint(0, maxShingleID) \n",
    "    \n",
    "            # Ensure that each random number is unique.\n",
    "            while randIndex in randList:\n",
    "                randIndex = random.randint(0, maxShingleID) \n",
    "        \n",
    "            # Add the random number to the list.\n",
    "            randList.append(randIndex)\n",
    "            k = k - 1\n",
    "        \n",
    "        return randList\n",
    "\n",
    "    # For each of the 'numHashes' hash functions, generate a different coefficient 'a' and 'b'.   \n",
    "    coeffA = pickRandomCoeffs(numHashes)\n",
    "    coeffB = pickRandomCoeffs(numHashes)\n",
    "\n",
    "\n",
    "    print ('Generating MinHash signatures for all documents...')\n",
    "\n",
    "    # List of documents represented as signature vectors\n",
    "    signatures = []\n",
    "\n",
    "    # Rather than generating a random permutation of all possible shingles, \n",
    "    # we'll just hash the IDs of the shingles that are *actually in the document*,\n",
    "    # then take the lowest resulting hash code value. This corresponds to the index \n",
    "    # of the first shingle that you would have encountered in the random order.\n",
    "\n",
    "    # For each tweet...\n",
    "    for tweetID in tweetNames:\n",
    "    \n",
    "        # Get the shingle set for this document.\n",
    "        shingleIDSet = tweetAsShingleSets[tweetID]\n",
    "    \n",
    "        # The resulting minhash signature for this document. \n",
    "        signature = []\n",
    "    \n",
    "        # For each of the random hash functions...\n",
    "        for i in range(0, numHashes):\n",
    "        \n",
    "            # For each of the shingles actually in the document, calculate its hash code\n",
    "            # using hash function 'i'. \n",
    "        \n",
    "            # Track the lowest hash ID seen. Initialize 'minHashCode' to be greater than\n",
    "            # the maximum possible value output by the hash.\n",
    "            minHashCode = nextPrime + 1\n",
    "        \n",
    "            # For each shingle in the document...\n",
    "            for shingleID in shingleIDSet:\n",
    "                # Evaluate the hash function.\n",
    "                hashCode = (coeffA[i] * shingleID + coeffB[i]) % nextPrime \n",
    "        \n",
    "                # Track the lowest hash code seen.\n",
    "                if hashCode < minHashCode:\n",
    "                    minHashCode = hashCode\n",
    "\n",
    "            # Add the smallest hash code value as component number 'i' of the signature.\n",
    "            signature.append(minHashCode)\n",
    "        # Store the MinHash signature for this document.\n",
    "        signatures.append(signature)\n",
    "\n",
    "\n",
    "    # =============================================================================\n",
    "    #                     Compare All Signatures\n",
    "    # =============================================================================  \n",
    "    print ('Comparing all signatures...')\n",
    "\n",
    "    # Creates a N x N matrix initialized to 0.\n",
    "    # For each of the test documents...\n",
    "    for i in range(0, numtweets):\n",
    "   \n",
    "        # Get the MinHash signature for document i.\n",
    "        signature1 = signatures[i]\n",
    "        # if i%100==0:\n",
    "        #     print(i)\n",
    "        \n",
    "        # For each of the other test documents...\n",
    "        for j in range(i + 1, numtweets):\n",
    "        \n",
    "            # Get the MinHash signature for document j.\n",
    "            try:\n",
    "                signature2 = signatures[j]\n",
    "            except:\n",
    "                print(\"warning on comparing signatures\")\n",
    "                continue\n",
    "        \n",
    "            count = 0\n",
    "            # Count the number of positions in the minhash signature which are equal.\n",
    "            for k in range(0, numHashes):\n",
    "                count = count + (signature1[k] == signature2[k])\n",
    "        \n",
    "            # Record the percentage of positions which matched.    \n",
    "            estJSim[getTriangleIndex(i, j)] = (count / numHashes)\n",
    "\n",
    "\n",
    "    # =============================================================================\n",
    "    #                   Display Similar Document Pairs\n",
    "    # =============================================================================  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    threshold = 0.5\n",
    "    # For each of the document pairs...\n",
    "    with open(output_data_folder_path+'cluster','w') as same_file, open(output_data_folder_path+'non_cluster','w') as unique_file:\n",
    "        same_dict = {}\n",
    "        same_list = []\n",
    "        for i in range(0, numtweets):\n",
    "            if not tweetNames[i] in same_list:\n",
    "                value = ''\n",
    "                for j in range(i + 1, numtweets):\n",
    "                    if not tweetNames[j] in same_list:\n",
    "                        # Retrieve the estimated similarity value for this pair.\n",
    "                        estJ = estJSim[getTriangleIndex(i, j)]\n",
    "                \n",
    "                        # If the similarity is above the threshold...\n",
    "                        if estJ > threshold:\n",
    "\n",
    "                            if tweetNames[i] in same_dict.keys():\n",
    "                                value = same_dict[tweetNames[i]]+':'+str(tweetNames[j])\n",
    "                                same_dict[tweetNames[i]] = value\n",
    "                                same_list.append(tweetNames[j])\n",
    "                            else:\n",
    "                                value = str(tweetNames[j])\n",
    "                                same_dict[tweetNames[i]] = value\n",
    "                                same_list.append(tweetNames[i])\n",
    "                                same_list.append(tweetNames[j])\n",
    "                # unique\n",
    "                if not value:\n",
    "                    unique_file.write(str(tweetNames[i])+':')             \n",
    "                            \n",
    "\n",
    "        \n",
    "        for k, v in same_dict.items():\n",
    "            same_file.write(k+':'+v+'\\n')\n",
    "\n",
    "\n",
    "    proc_date = proc_date+datetime.timedelta(days=1)\n",
    "    if proc_date == end_date_datetime:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30849d80-fdfb-4527-be7e-dbba9084410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import datetime\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "url_pattern = re.compile(r'http\\S+')\n",
    "hashtag_pattern = re.compile(r'#\\S+')\n",
    "mention_pattern = re.compile(r'@\\S+')\n",
    "rule = re.compile(r'[^a-zA-Z\\s]')\n",
    "\n",
    "\n",
    "def extract_text(tw):\n",
    "    if is_extended(tw):\n",
    "        return tw[\"extended_tweet\"][\"full_text\"]\n",
    "    else:\n",
    "        return tw[\"text\"]\n",
    "\n",
    "def is_en(tw):\n",
    "    return tw[\"lang\"]==\"en\"\n",
    "\n",
    "\n",
    "def get_tweet_id(tw):\n",
    "    return tw[\"id_str\"]\n",
    "\n",
    "def get_user_id(tw):\n",
    "    return tw[\"user\"][\"id_str\"]\n",
    "\n",
    "def is_extended(tw):\n",
    "    if \"extended_tweet\" in tw:\n",
    "        return True\n",
    "    else:\n",
    "        return False    \n",
    "\n",
    "def text_filter(text):\n",
    "    if text == None:\n",
    "        return ''\n",
    "\n",
    "    new_text = text\n",
    "\n",
    "    \n",
    "    #filter url\n",
    "    new_text = re.sub(url_pattern,'',new_text)\n",
    "\n",
    "    #filter hashtag\n",
    "    new_text = re.sub(hashtag_pattern,'',new_text)\n",
    "\n",
    "    #filter mention\n",
    "    new_text = re.sub(mention_pattern,' ',new_text)\n",
    "\n",
    "    # filter \\t \\n\n",
    "    new_text = re.sub('\\t|\\n|\\r|\\v',' ', new_text)\n",
    "    # filter multi spaces\n",
    "    new_text = re.sub(' +', ' ', new_text)\n",
    "\n",
    "    # remove space of head or tail\n",
    "    new_text = re.sub('^ | $','', new_text)\n",
    "\n",
    "    # keep number and char\n",
    "    new_text = re.sub(rule, '', new_text)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "start_date = \"20201117\"\n",
    "end_date = \"20210521\"\n",
    "start_date_datetime = datetime.datetime.strptime(start_date, '%Y%m%d')\n",
    "end_date_datetime = datetime.datetime.strptime(end_date, '%Y%m%d')\n",
    "proc_date = start_date_datetime\n",
    "duration = 300   # t\n",
    "\n",
    "data_check_list = os.listdir(\"Data/\")\n",
    "data_check_dic = {i:1 for i in data_check_list}\n",
    "\n",
    "for _ in range(duration):\n",
    "    # process the data in this date\n",
    "    proc_date_str = proc_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    input_data_folder_path = \"Data/\"+proc_date_str+\"/\"\n",
    "    output_data_folder_path =   \"Tmp/\"+proc_date_str+\"/\"\n",
    "\n",
    "\n",
    "    if not proc_date_str in data_check_dic.keys():\n",
    "        proc_date = proc_date+datetime.timedelta(days=1)\n",
    "        if proc_date == end_date_datetime:\n",
    "            break\n",
    "        continue\n",
    "    \n",
    "    if not os.path.exists(output_data_folder_path):\n",
    "        os.makedirs(output_data_folder_path)\n",
    "\n",
    "    output_data_file = output_data_folder_path+\"tweet_text\"\n",
    "    with open(output_data_file, 'w', encoding='utf-8') as file_out:\n",
    "\n",
    "\n",
    "        for filename in os.listdir(input_data_folder_path):\n",
    "            input_data_path = input_data_folder_path+filename\n",
    "\n",
    "            with open(input_data_path, 'r', encoding='utf-8', errors='ignore') as file_in:\n",
    "                \n",
    "        \n",
    "                for line in file_in:\n",
    "                    try:\n",
    "                        tweet = json.loads(line)\n",
    "                    except:\n",
    "                        print(tweet)\n",
    "                        continue\n",
    "                    if is_en(tweet):\n",
    "                        tweet_id = get_tweet_id(tweet)\n",
    "                        user_id = get_user_id(tweet)\n",
    "                        text = extract_text(tweet)\n",
    "                        text = text_filter(text)\n",
    "                        # if len(text) < 20:\n",
    "                        #     continue\n",
    "                        file_out.write(tweet_id+'\\t'+user_id+'\\t'+text+'\\t')\n",
    "                        file_out.write('\\n')\n",
    "                        file_out.flush()\n",
    "\n",
    "\n",
    "    proc_date = proc_date+datetime.timedelta(days=1)\n",
    "    if proc_date == end_date_datetime:\n",
    "        break\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee32bec6-e3e1-488b-9433-f092be0c2797",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Tmp/2022-01-01/cluster'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m output_data_file \u001b[38;5;241m=\u001b[39m output_data_folder_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabeled_tweets.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m cluster_reverse \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(input_data_cluster, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file_in:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file_in:\n\u001b[1;32m     44\u001b[0m         line_split \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml_course/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Tmp/2022-01-01/cluster'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "start_date = \"20220101\"\n",
    "end_date = \"20220102\"\n",
    "start_date_datetime = datetime.datetime.strptime(start_date, '%Y%m%d')\n",
    "end_date_datetime = datetime.datetime.strptime(end_date, '%Y%m%d')\n",
    "proc_date = start_date_datetime\n",
    "duration = 300   #\n",
    "\n",
    "data_check_list = os.listdir(\"Data/\")\n",
    "data_check_dic = {i:1 for i in data_check_list}\n",
    "\n",
    "\n",
    "for _ in range(duration):\n",
    "    # process the data in this date\n",
    "    proc_date_str = proc_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    input_data_tmp_path = \"Tmp/\"+proc_date_str+\"/\"\n",
    "    input_data_folder_path = \"Cluster/\"+proc_date_str+\"/\"\n",
    "    output_data_folder_path =  \"Label/\"+proc_date_str+\"/\"\n",
    "\n",
    "    if not proc_date_str in data_check_dic.keys():\n",
    "        proc_date = proc_date+datetime.timedelta(days=1)\n",
    "        if proc_date == end_date_datetime:\n",
    "            break\n",
    "        continue\n",
    "\n",
    "    if not os.path.exists(output_data_folder_path):\n",
    "        os.makedirs(output_data_folder_path)\n",
    "\n",
    "    input_data_text = input_data_tmp_path+\"tweet_text\"\n",
    "    input_data_cluster = input_data_tmp_path+'cluster'\n",
    "    input_data_label = input_data_folder_path+'tweets.txt'\n",
    "    output_data_file = output_data_folder_path+\"labeled_tweets.txt\"\n",
    "\n",
    "    cluster_reverse = {}\n",
    "    with open(input_data_cluster, 'r') as file_in:\n",
    "        for line in file_in:\n",
    "            line_split = line.strip().split(\":\")\n",
    "            for lc in line_split:\n",
    "                cluster_reverse[lc] = line_split[0]\n",
    "\n",
    "    label_dic = {}\n",
    "    with open(input_data_label, 'r') as file_in:\n",
    "        for line in file_in:\n",
    "            line_split = line.strip().split(\"\\t\")\n",
    "            if not line_split[0] == 'N':\n",
    "                label_dic[line_split[2]] = line_split[0]\n",
    "\n",
    "    with open(input_data_text, 'r') as file_in, \\\n",
    "        open(output_data_file, 'w') as file_out:\n",
    "        for line in file_in:\n",
    "            line_split = line.strip().split(\"\\t\")\n",
    "            tweet_id = line_split[0]\n",
    "            user_id = line_split[1]\n",
    "            tweet_text = line_split[2]\n",
    "\n",
    "            if not tweet_id in cluster_reverse.keys():\n",
    "                continue\n",
    "            cluster_id = cluster_reverse[tweet_id]\n",
    "\n",
    "            if not cluster_id in label_dic.keys():\n",
    "                continue\n",
    "\n",
    "            label = label_dic[cluster_id]\n",
    "      \n",
    "\n",
    "            file_out.write(tweet_id+'\\t')\n",
    "            file_out.write(user_id+'\\t')\n",
    "            file_out.write(tweet_text+'\\t')\n",
    "            file_out.write(label+'\\n')\n",
    "            file_out.flush()\n",
    "\n",
    "    proc_date = proc_date+datetime.timedelta(days=1)\n",
    "    if proc_date == end_date_datetime:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2aecf434-34a1-4fb0-a155-8af3f8981438",
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: 'Data/2022-01-01/.ipynb_checkpoints'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(input_data_folder_path):\n\u001b[1;32m     38\u001b[0m     input_data_path \u001b[38;5;241m=\u001b[39m input_data_folder_path\u001b[38;5;241m+\u001b[39mfilename\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(input_data_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file_in:\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file_in:\n\u001b[1;32m     43\u001b[0m             \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml_course/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: 'Data/2022-01-01/.ipynb_checkpoints'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "from tool.Twitter import Tweet\n",
    "\n",
    "start_date = \"20220101\"\n",
    "end_date = \"20220102\"\n",
    "start_date_datetime = datetime.datetime.strptime(start_date, '%Y%m%d')\n",
    "end_date_datetime = datetime.datetime.strptime(end_date, '%Y%m%d')\n",
    "proc_date = start_date_datetime\n",
    "duration = 300   # t\n",
    "\n",
    "data_check_list = os.listdir(\"Data/\")\n",
    "data_check_dic = {i:1 for i in data_check_list}\n",
    "\n",
    "for _ in range(duration):\n",
    "    # process the data in this date\n",
    "    proc_date_str = proc_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    input_data_folder_path = \"Data/\"+proc_date_str+\"/\"\n",
    "    output_data_folder_path =   \"Tmp/\"+proc_date_str+\"/\"\n",
    "\n",
    "\n",
    "    if not proc_date_str in data_check_dic.keys():\n",
    "        proc_date = proc_date+datetime.timedelta(days=1)\n",
    "        if proc_date == end_date_datetime:\n",
    "            break\n",
    "        continue\n",
    "    \n",
    "    if not os.path.exists(output_data_folder_path):\n",
    "        os.makedirs(output_data_folder_path)\n",
    "\n",
    "    output_data_file = output_data_folder_path+\"tweet_feature\"\n",
    "    with open(output_data_file, 'w', encoding='utf-8') as file_out:\n",
    "\n",
    "\n",
    "        for filename in os.listdir(input_data_folder_path):\n",
    "            input_data_path = input_data_folder_path+filename\n",
    "\n",
    "            with open(input_data_path, 'r', encoding='utf-8', errors='ignore') as file_in:\n",
    "                \n",
    "                for line in file_in:\n",
    "                    try:\n",
    "                        tweet = json.loads(line)\n",
    "                        tweet_obj = Tweet(tweet)\n",
    "                    except:\n",
    "                        print(tweet)\n",
    "                        continue\n",
    "                    \n",
    "                    if not tweet_obj.is_en():\n",
    "                        continue\n",
    "\n",
    "                    tweet_id = tweet_obj.get_id()\n",
    "                    user_id = tweet_obj.user.id_str\n",
    "                    feature_list = tweet_obj.get_tweet_features()\n",
    "                    file_out.write(tweet_id+'\\t'+user_id+'\\t')\n",
    "\n",
    "                    for f in feature_list:\n",
    "                        file_out.write(str(f))\n",
    "                        file_out.write('\\t')\n",
    "                    file_out.write('\\n')\n",
    "                    file_out.flush()\n",
    "                    \n",
    "    proc_date = proc_date+datetime.timedelta(days=1)\n",
    "    if proc_date == end_date_datetime:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e121c790-83dd-4734-97ed-2a166edb8d0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: 'Data/2022-01-01/.ipynb_checkpoints'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 98\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(input_data_folder_path):\n\u001b[1;32m     96\u001b[0m     input_data_path \u001b[38;5;241m=\u001b[39m input_data_folder_path\u001b[38;5;241m+\u001b[39mfilename\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(input_data_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file_in:\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file_in:\n\u001b[1;32m    102\u001b[0m             \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml_course/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: 'Data/2022-01-01/.ipynb_checkpoints'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import datetime\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "url_pattern = re.compile(r'http\\S+')\n",
    "hashtag_pattern = re.compile(r'#\\S+')\n",
    "mention_pattern = re.compile(r'@\\S+')\n",
    "rule = re.compile(r'[^a-zA-Z\\s]')\n",
    "\n",
    "\n",
    "def extract_text(tw):\n",
    "    if is_extended(tw):\n",
    "        return tw[\"extended_tweet\"][\"full_text\"]\n",
    "    else:\n",
    "        return tw[\"text\"]\n",
    "\n",
    "def is_en(tw):\n",
    "    return tw[\"lang\"]==\"en\"\n",
    "\n",
    "\n",
    "def get_tweet_id(tw):\n",
    "    return tw[\"id_str\"]\n",
    "\n",
    "def get_user_id(tw):\n",
    "    return tw[\"user\"][\"id_str\"]\n",
    "\n",
    "def is_extended(tw):\n",
    "    if \"extended_tweet\" in tw:\n",
    "        return True\n",
    "    else:\n",
    "        return False    \n",
    "\n",
    "def text_filter(text):\n",
    "    if text == None:\n",
    "        return ''\n",
    "\n",
    "    new_text = text\n",
    "\n",
    "    \n",
    "    #filter url\n",
    "    new_text = re.sub(url_pattern,'',new_text)\n",
    "\n",
    "    #filter hashtag\n",
    "    new_text = re.sub(hashtag_pattern,'',new_text)\n",
    "\n",
    "    #filter mention\n",
    "    new_text = re.sub(mention_pattern,' ',new_text)\n",
    "\n",
    "    # filter \\t \\n\n",
    "    new_text = re.sub('\\t|\\n|\\r|\\v',' ', new_text)\n",
    "    # filter multi spaces\n",
    "    new_text = re.sub(' +', ' ', new_text)\n",
    "\n",
    "    # remove space of head or tail\n",
    "    new_text = re.sub('^ | $','', new_text)\n",
    "\n",
    "    # keep number and char\n",
    "    new_text = re.sub(rule, '', new_text)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "start_date = \"20220101\"\n",
    "end_date = \"20220102\"\n",
    "start_date_datetime = datetime.datetime.strptime(start_date, '%Y%m%d')\n",
    "end_date_datetime = datetime.datetime.strptime(end_date, '%Y%m%d')\n",
    "proc_date = start_date_datetime\n",
    "duration = 300   # t\n",
    "\n",
    "data_check_list = os.listdir(\"Data/\")\n",
    "data_check_dic = {i:1 for i in data_check_list}\n",
    "\n",
    "for _ in range(duration):\n",
    "    # process the data in this date\n",
    "    proc_date_str = proc_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    input_data_folder_path = \"Data/\"+proc_date_str+\"/\"\n",
    "    output_data_folder_path =   \"Tmp/\"+proc_date_str+\"/\"\n",
    "\n",
    "\n",
    "    if not proc_date_str in data_check_dic.keys():\n",
    "        proc_date = proc_date+datetime.timedelta(days=1)\n",
    "        if proc_date == end_date_datetime:\n",
    "            break\n",
    "        continue\n",
    "    \n",
    "    if not os.path.exists(output_data_folder_path):\n",
    "        os.makedirs(output_data_folder_path)\n",
    "\n",
    "    output_data_file = output_data_folder_path+\"tweet_text\"\n",
    "    with open(output_data_file, 'w', encoding='utf-8') as file_out:\n",
    "\n",
    "\n",
    "        for filename in os.listdir(input_data_folder_path):\n",
    "            input_data_path = input_data_folder_path+filename\n",
    "\n",
    "            with open(input_data_path, 'r', encoding='utf-8', errors='ignore') as file_in:\n",
    "                \n",
    "        \n",
    "                for line in file_in:\n",
    "                    try:\n",
    "                        tweet = json.loads(line)\n",
    "                    except:\n",
    "                        print(tweet)\n",
    "                        continue\n",
    "                    if is_en(tweet):\n",
    "                        tweet_id = get_tweet_id(tweet)\n",
    "                        user_id = get_user_id(tweet)\n",
    "                        text = extract_text(tweet)\n",
    "                        text = text_filter(text)\n",
    "                        # if len(text) < 20:\n",
    "                        #     continue\n",
    "                        file_out.write(tweet_id+'\\t'+user_id+'\\t'+text+'\\t')\n",
    "                        file_out.write('\\n')\n",
    "                        file_out.flush()\n",
    "\n",
    "\n",
    "    proc_date = proc_date+datetime.timedelta(days=1)\n",
    "    if proc_date == end_date_datetime:\n",
    "        break\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37bb9030-7807-454b-83ce-8c963609d488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-01\n",
      "Shingling tweets...\n",
      "Generating random hash functions...\n",
      "Generating MinHash signatures for all documents...\n",
      "Comparing all signatures...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import datetime\n",
    "import binascii\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "start_date = \"20220101\"\n",
    "end_date = \"20220102\"\n",
    "start_date_datetime = datetime.datetime.strptime(start_date, '%Y%m%d')\n",
    "end_date_datetime = datetime.datetime.strptime(end_date, '%Y%m%d')\n",
    "proc_date = start_date_datetime\n",
    "duration = 300   #\n",
    "future_days = 5\n",
    "\n",
    "data_check_list = os.listdir(\"Data/\")\n",
    "data_check_dic = {i:1 for i in data_check_list}\n",
    "\n",
    "for _ in range(duration):\n",
    "    # process the data in this date\n",
    "    proc_date_str = proc_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    print(proc_date_str)\n",
    "\n",
    "    input_data_folder_path = \"Tmp/\"+proc_date_str+\"/\"\n",
    "    output_data_folder_path =   \"Tmp/\"+proc_date_str+\"/\"\n",
    "\n",
    "    if not proc_date_str in data_check_dic.keys():\n",
    "        proc_date = proc_date+datetime.timedelta(days=1)\n",
    "        if proc_date == end_date_datetime:\n",
    "            break\n",
    "        continue\n",
    "\n",
    "    input_data_file_list = []\n",
    "    for i in range(future_days):\n",
    "        future_date =  proc_date+datetime.timedelta(days=i)\n",
    "        future_date_str = future_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        if not future_date_str in data_check_dic.keys():\n",
    "            continue\n",
    "        future_data_folder_path = \"Tmp/\"+future_date_str+\"/\"\n",
    "        future_data_file = future_data_folder_path+\"tweet_text\"\n",
    "        input_data_file_list.append(future_data_file)\n",
    "\n",
    "\n",
    "    numtweets = 0\n",
    "    numHashes = 10\n",
    "    curtweets = 0\n",
    "\n",
    "    print('Shingling tweets...')\n",
    "\n",
    "    # The current shingle ID value to assign to the next new shingle we \n",
    "    # encounter. When a shingle gets added to the dictionary, we'll increment this\n",
    "    # value.\n",
    "    curShingleID = 0\n",
    "\n",
    "    # Create a dictionary of the articles, mapping the article identifier (e.g., \n",
    "    # \"t8470\") to the list of shingle IDs that appear in the document.\n",
    "    tweetAsShingleSets = {}\n",
    "    tweetNames = []\n",
    "    totalShingles = 0\n",
    "\n",
    "    # if not os.path.exists(output_data_folder_path):\n",
    "    #     os.makedirs(output_data_folder_path)\n",
    "\n",
    "    # tweet_limit = 10000\n",
    "    for input_data_file in input_data_file_list:\n",
    "        with open(input_data_file, 'r', encoding='utf-8', errors='ignore') as file_in:\n",
    "            for line in file_in:\n",
    "                # tweet_limit -= 1\n",
    "                # if tweet_limit < 0:\n",
    "                #     break\n",
    "                \n",
    "                tweet_id = line.strip().split('\\t')[0]\n",
    "                try:\n",
    "                    tweet_text =  line.strip().split('\\t')[2]\n",
    "                except:\n",
    "                    continue\n",
    "                tweet_token = tweet_text.strip().split()\n",
    "                \n",
    "                # Maintain a list of all tweet IDs.\n",
    "                tweetNames.append(tweet_id)\n",
    "\n",
    "                curtweets = curtweets + 1\n",
    "\n",
    "                # 'shinglesInDoc' will hold all of the unique shingle IDs present in the \n",
    "                # current document. If a shingle ID occurs multiple times in the document,\n",
    "                # it will only appear once in the set (this is a property of Python sets).\n",
    "                shinglesInTweet = set()\n",
    "\n",
    "                # For each word in the document...\n",
    "                for index in range(0, len(tweet_token) - 2):\n",
    "                    # Construct the shingle text by combining three words together.\n",
    "                    shingle =tweet_token[index] + ' ' + tweet_token[index + 1] + ' ' + tweet_token[index + 2]\n",
    "                    # shingle = tweet_text[index] + ' ' + tweet_text[index + 1] + ' ' + tweet_text[index + 2]\n",
    "    \n",
    "\n",
    "                    # Hash the shingle to a 32-bit integer.\n",
    "                    crc = binascii.crc32(bytes(shingle, encoding=\"ascii\")) & 0xffffffff\n",
    "\n",
    "            \n",
    "                    # Add the hash value to the list of shingles for the current document. \n",
    "                    # Note that set objects will only add the value to the set if the set \n",
    "                    # doesn't already contain it. \n",
    "                    shinglesInTweet.add(crc)\n",
    "\n",
    "                # Store the completed list of shingles for this document in the dictionary.\n",
    "                tweetAsShingleSets[tweet_id] = shinglesInTweet\n",
    "    numtweets = curtweets\n",
    "\n",
    "\n",
    "    # =============================================================================\n",
    "    #                     Define Triangle Matrices\n",
    "    # =============================================================================\n",
    "\n",
    "    # Define virtual Triangle matrices to hold the similarity values. For storing\n",
    "    # similarities between pairs, we only need roughly half the elements of a full\n",
    "    # matrix. Using a triangle matrix requires less than half the memory of a full\n",
    "    # matrix, and can protect the programmer from inadvertently accessing one of\n",
    "    # the empty/invalid cells of a full matrix.\n",
    "\n",
    "    # Calculate the number of elements needed in our triangle matrix\n",
    "    numElems = int(numtweets * (numtweets - 1) / 2)\n",
    "\n",
    "    # Initialize two empty lists to store the similarity values. \n",
    "    # 'JSim' will be for the actual Jaccard Similarity values. \n",
    "    # 'estJSim' will be for the estimated Jaccard Similarities found by comparing\n",
    "    # the MinHash signatures.\n",
    "    JSim = [0 for x in range(numElems)]\n",
    "    estJSim = [0 for x in range(numElems)]\n",
    "\n",
    "    def getTriangleIndex(i, j):\n",
    "    # If i == j that's an error.\n",
    "        if i == j:\n",
    "            sys.stderr.write(\"Can't access triangle matrix with i == j\")\n",
    "            sys.exit(1)\n",
    "        # If j < i just swap the values.\n",
    "        if j < i:\n",
    "            i, j = j, i\n",
    "            # temp = i\n",
    "            # i = j\n",
    "            # j = temp\n",
    "\n",
    "        # Calculate the index within the triangular array.\n",
    "        # This fancy indexing scheme is taken from pg. 211 of:\n",
    "        # http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n",
    "        # But I adapted it for a 0-based index.\n",
    "        # Note: The division by two should not truncate, it\n",
    "        #       needs to be a float. \n",
    "        k = int(i * (numtweets - (i + 1) / 2.0) + j - i) - 1\n",
    "\n",
    "        return k\n",
    "\n",
    "    # =============================================================================\n",
    "    #                 Generate MinHash Signatures\n",
    "    # =============================================================================\n",
    "    print ('Generating random hash functions...')\n",
    "\n",
    "    # Record the maximum shingle ID that we assigned.\n",
    "    maxShingleID = 2**32-1\n",
    "\n",
    "    # We need the next largest prime number above 'maxShingleID'.\n",
    "    # I looked this value up here: \n",
    "    # http://compoasso.free.fr/primelistweb/page/prime/liste_online_en.php\n",
    "    nextPrime = 4294967311\n",
    "\n",
    "\n",
    "    # Our random hash function will take the form of:\n",
    "    #   h(x) = (a*x + b) % c\n",
    "    # Where 'x' is the input value, 'a' and 'b' are random coefficients, and 'c' is\n",
    "    # a prime number just greater than maxShingleID.\n",
    "\n",
    "    # Generate a list of 'k' random coefficients for the random hash functions,\n",
    "    # while ensuring that the same value does not appear multiple times in the \n",
    "    # list.\n",
    "    def pickRandomCoeffs(k):\n",
    "        # Create a list of 'k' random values.\n",
    "        randList = []\n",
    "    \n",
    "        while k > 0:\n",
    "            # Get a random shingle ID.\n",
    "            randIndex = random.randint(0, maxShingleID) \n",
    "    \n",
    "            # Ensure that each random number is unique.\n",
    "            while randIndex in randList:\n",
    "                randIndex = random.randint(0, maxShingleID) \n",
    "        \n",
    "            # Add the random number to the list.\n",
    "            randList.append(randIndex)\n",
    "            k = k - 1\n",
    "        \n",
    "        return randList\n",
    "\n",
    "    # For each of the 'numHashes' hash functions, generate a different coefficient 'a' and 'b'.   \n",
    "    coeffA = pickRandomCoeffs(numHashes)\n",
    "    coeffB = pickRandomCoeffs(numHashes)\n",
    "\n",
    "\n",
    "    print ('Generating MinHash signatures for all documents...')\n",
    "\n",
    "    # List of documents represented as signature vectors\n",
    "    signatures = []\n",
    "\n",
    "    # Rather than generating a random permutation of all possible shingles, \n",
    "    # we'll just hash the IDs of the shingles that are *actually in the document*,\n",
    "    # then take the lowest resulting hash code value. This corresponds to the index \n",
    "    # of the first shingle that you would have encountered in the random order.\n",
    "\n",
    "    # For each tweet...\n",
    "    for tweetID in tweetNames:\n",
    "    \n",
    "        # Get the shingle set for this document.\n",
    "        shingleIDSet = tweetAsShingleSets[tweetID]\n",
    "    \n",
    "        # The resulting minhash signature for this document. \n",
    "        signature = []\n",
    "    \n",
    "        # For each of the random hash functions...\n",
    "        for i in range(0, numHashes):\n",
    "        \n",
    "            # For each of the shingles actually in the document, calculate its hash code\n",
    "            # using hash function 'i'. \n",
    "        \n",
    "            # Track the lowest hash ID seen. Initialize 'minHashCode' to be greater than\n",
    "            # the maximum possible value output by the hash.\n",
    "            minHashCode = nextPrime + 1\n",
    "        \n",
    "            # For each shingle in the document...\n",
    "            for shingleID in shingleIDSet:\n",
    "                # Evaluate the hash function.\n",
    "                hashCode = (coeffA[i] * shingleID + coeffB[i]) % nextPrime \n",
    "        \n",
    "                # Track the lowest hash code seen.\n",
    "                if hashCode < minHashCode:\n",
    "                    minHashCode = hashCode\n",
    "\n",
    "            # Add the smallest hash code value as component number 'i' of the signature.\n",
    "            signature.append(minHashCode)\n",
    "        # Store the MinHash signature for this document.\n",
    "        signatures.append(signature)\n",
    "\n",
    "\n",
    "    # =============================================================================\n",
    "    #                     Compare All Signatures\n",
    "    # =============================================================================  \n",
    "    print ('Comparing all signatures...')\n",
    "\n",
    "    # Creates a N x N matrix initialized to 0.\n",
    "    # For each of the test documents...\n",
    "    for i in range(0, numtweets):\n",
    "   \n",
    "        # Get the MinHash signature for document i.\n",
    "        signature1 = signatures[i]\n",
    "        # if i%100==0:\n",
    "        #     print(i)\n",
    "        \n",
    "        # For each of the other test documents...\n",
    "        for j in range(i + 1, numtweets):\n",
    "        \n",
    "            # Get the MinHash signature for document j.\n",
    "            try:\n",
    "                signature2 = signatures[j]\n",
    "            except:\n",
    "                print(\"warning on comparing signatures\")\n",
    "                continue\n",
    "        \n",
    "            count = 0\n",
    "            # Count the number of positions in the minhash signature which are equal.\n",
    "            for k in range(0, numHashes):\n",
    "                count = count + (signature1[k] == signature2[k])\n",
    "        \n",
    "            # Record the percentage of positions which matched.    \n",
    "            estJSim[getTriangleIndex(i, j)] = (count / numHashes)\n",
    "\n",
    "\n",
    "    # =============================================================================\n",
    "    #                   Display Similar Document Pairs\n",
    "    # =============================================================================  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    threshold = 0.5\n",
    "    # For each of the document pairs...\n",
    "    with open(output_data_folder_path+'cluster','w') as same_file, open(output_data_folder_path+'non_cluster','w') as unique_file:\n",
    "        same_dict = {}\n",
    "        same_list = []\n",
    "        for i in range(0, numtweets):\n",
    "            if not tweetNames[i] in same_list:\n",
    "                value = ''\n",
    "                for j in range(i + 1, numtweets):\n",
    "                    if not tweetNames[j] in same_list:\n",
    "                        # Retrieve the estimated similarity value for this pair.\n",
    "                        estJ = estJSim[getTriangleIndex(i, j)]\n",
    "                \n",
    "                        # If the similarity is above the threshold...\n",
    "                        if estJ > threshold:\n",
    "\n",
    "                            if tweetNames[i] in same_dict.keys():\n",
    "                                value = same_dict[tweetNames[i]]+':'+str(tweetNames[j])\n",
    "                                same_dict[tweetNames[i]] = value\n",
    "                                same_list.append(tweetNames[j])\n",
    "                            else:\n",
    "                                value = str(tweetNames[j])\n",
    "                                same_dict[tweetNames[i]] = value\n",
    "                                same_list.append(tweetNames[i])\n",
    "                                same_list.append(tweetNames[j])\n",
    "                # unique\n",
    "                if not value:\n",
    "                    unique_file.write(str(tweetNames[i])+':')             \n",
    "                            \n",
    "\n",
    "        \n",
    "        for k, v in same_dict.items():\n",
    "            same_file.write(k+':'+v+'\\n')\n",
    "\n",
    "\n",
    "    proc_date = proc_date+datetime.timedelta(days=1)\n",
    "    if proc_date == end_date_datetime:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c5ff9a2-d5ca-4b2d-9f46-0a3d833220ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "start_date = \"20220101\"\n",
    "end_date = \"20220102\"\n",
    "start_date_datetime = datetime.datetime.strptime(start_date, '%Y%m%d')\n",
    "end_date_datetime = datetime.datetime.strptime(end_date, '%Y%m%d')\n",
    "proc_date = start_date_datetime\n",
    "duration = 300   #\n",
    "\n",
    "data_check_list = os.listdir(\"Data/\")\n",
    "data_check_dic = {i:1 for i in data_check_list}\n",
    "\n",
    "for _ in range(duration):\n",
    "    # process the data in this date\n",
    "    proc_date_str = proc_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    input_data_folder_path = \"Tmp/\"+proc_date_str+\"/\"\n",
    "    output_data_folder_path =  \"Cluster/\"+proc_date_str+\"/\"\n",
    "\n",
    "    if not proc_date_str in data_check_dic.keys():\n",
    "        proc_date = proc_date+datetime.timedelta(days=1)\n",
    "        if proc_date == end_date_datetime:\n",
    "            break\n",
    "        continue\n",
    "\n",
    "    if not os.path.exists(output_data_folder_path):\n",
    "        os.makedirs(output_data_folder_path)\n",
    "\n",
    "\n",
    "\n",
    "    input_data_cluster = input_data_folder_path+'cluster'\n",
    "    input_data_text = input_data_folder_path+'tweet_text'\n",
    "    output_data_file = output_data_folder_path+\"tweets.txt\"\n",
    "\n",
    "    cluster_dic = {}\n",
    "    with open (input_data_cluster, 'r') as file_in:\n",
    "        for line in file_in:\n",
    "            line_split = line.strip().split(\":\")\n",
    "\n",
    "            cluster_dic[line_split[0]] = len(line_split)\n",
    "\n",
    "    with open(input_data_text, 'r') as file_in, \\\n",
    "        open(output_data_file, 'w') as file_out:\n",
    "        for line in file_in:\n",
    "            line_split = line.strip().split(\"\\t\")\n",
    "            tweet_id = line_split[0]\n",
    "            user_id = line_split[1]\n",
    "            tweet_text = line_split[2]\n",
    "\n",
    "            if tweet_id in cluster_dic.keys():\n",
    "                file_out.write('N'+'\\t')\n",
    "                file_out.write(str(cluster_dic[tweet_id])+'\\t')\n",
    "                file_out.write(tweet_id+'\\t')\n",
    "                file_out.write(user_id+'\\t')\n",
    "                file_out.write(tweet_text+'\\n')\n",
    "                file_out.flush()\n",
    "\n",
    "    proc_date = proc_date+datetime.timedelta(days=1)\n",
    "    if proc_date == end_date_datetime:\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "faee0083-cadf-4c24-91d9-84d8af07ddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "start_date = \"20220101\"\n",
    "end_date = \"20220102\"\n",
    "start_date_datetime = datetime.datetime.strptime(start_date, '%Y%m%d')\n",
    "end_date_datetime = datetime.datetime.strptime(end_date, '%Y%m%d')\n",
    "proc_date = start_date_datetime\n",
    "duration = 300   #\n",
    "\n",
    "data_check_list = os.listdir(\"Data/\")\n",
    "data_check_dic = {i:1 for i in data_check_list}\n",
    "\n",
    "\n",
    "for _ in range(duration):\n",
    "    # process the data in this date\n",
    "    proc_date_str = proc_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    input_data_tmp_path = \"Tmp/\"+proc_date_str+\"/\"\n",
    "    input_data_folder_path = \"Cluster/\"+proc_date_str+\"/\"\n",
    "    output_data_folder_path =  \"Label/\"+proc_date_str+\"/\"\n",
    "\n",
    "    if not proc_date_str in data_check_dic.keys():\n",
    "        proc_date = proc_date+datetime.timedelta(days=1)\n",
    "        if proc_date == end_date_datetime:\n",
    "            break\n",
    "        continue\n",
    "\n",
    "    if not os.path.exists(output_data_folder_path):\n",
    "        os.makedirs(output_data_folder_path)\n",
    "\n",
    "    input_data_text = input_data_tmp_path+\"tweet_text\"\n",
    "    input_data_cluster = input_data_tmp_path+'cluster'\n",
    "    input_data_label = input_data_folder_path+'tweets.txt'\n",
    "    output_data_file = output_data_folder_path+\"labeled_tweets.txt\"\n",
    "\n",
    "    cluster_reverse = {}\n",
    "    with open(input_data_cluster, 'r') as file_in:\n",
    "        for line in file_in:\n",
    "            line_split = line.strip().split(\":\")\n",
    "            for lc in line_split:\n",
    "                cluster_reverse[lc] = line_split[0]\n",
    "\n",
    "    label_dic = {}\n",
    "    with open(input_data_label, 'r') as file_in:\n",
    "        for line in file_in:\n",
    "            line_split = line.strip().split(\"\\t\")\n",
    "            if not line_split[0] == 'N':\n",
    "                label_dic[line_split[2]] = line_split[0]\n",
    "\n",
    "    with open(input_data_text, 'r') as file_in, \\\n",
    "        open(output_data_file, 'w') as file_out:\n",
    "        for line in file_in:\n",
    "            line_split = line.strip().split(\"\\t\")\n",
    "            tweet_id = line_split[0]\n",
    "            user_id = line_split[1]\n",
    "            tweet_text = line_split[2]\n",
    "\n",
    "            if not tweet_id in cluster_reverse.keys():\n",
    "                continue\n",
    "            cluster_id = cluster_reverse[tweet_id]\n",
    "\n",
    "            if not cluster_id in label_dic.keys():\n",
    "                continue\n",
    "\n",
    "            label = label_dic[cluster_id]\n",
    "      \n",
    "\n",
    "            file_out.write(tweet_id+'\\t')\n",
    "            file_out.write(user_id+'\\t')\n",
    "            file_out.write(tweet_text+'\\t')\n",
    "            file_out.write(label+'\\n')\n",
    "            file_out.flush()\n",
    "\n",
    "    proc_date = proc_date+datetime.timedelta(days=1)\n",
    "    if proc_date == end_date_datetime:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33e20fe8-8d13-4706-a3ba-c171cd8090ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: 'Data/2022-01-01/.ipynb_checkpoints'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(input_data_folder_path):\n\u001b[1;32m     38\u001b[0m     input_data_path \u001b[38;5;241m=\u001b[39m input_data_folder_path\u001b[38;5;241m+\u001b[39mfilename\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(input_data_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file_in:\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file_in:\n\u001b[1;32m     43\u001b[0m             \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml_course/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: 'Data/2022-01-01/.ipynb_checkpoints'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "from tool.Twitter import Tweet\n",
    "\n",
    "start_date = \"20220101\"\n",
    "end_date = \"20220102\"\n",
    "start_date_datetime = datetime.datetime.strptime(start_date, '%Y%m%d')\n",
    "end_date_datetime = datetime.datetime.strptime(end_date, '%Y%m%d')\n",
    "proc_date = start_date_datetime\n",
    "duration = 300   # t\n",
    "\n",
    "data_check_list = os.listdir(\"Data/\")\n",
    "data_check_dic = {i:1 for i in data_check_list}\n",
    "\n",
    "for _ in range(duration):\n",
    "    # process the data in this date\n",
    "    proc_date_str = proc_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    input_data_folder_path = \"Data/\"+proc_date_str+\"/\"\n",
    "    output_data_folder_path =   \"Tmp/\"+proc_date_str+\"/\"\n",
    "\n",
    "\n",
    "    if not proc_date_str in data_check_dic.keys():\n",
    "        proc_date = proc_date+datetime.timedelta(days=1)\n",
    "        if proc_date == end_date_datetime:\n",
    "            break\n",
    "        continue\n",
    "    \n",
    "    if not os.path.exists(output_data_folder_path):\n",
    "        os.makedirs(output_data_folder_path)\n",
    "\n",
    "    output_data_file = output_data_folder_path+\"tweet_feature\"\n",
    "    with open(output_data_file, 'w', encoding='utf-8') as file_out:\n",
    "\n",
    "\n",
    "        for filename in os.listdir(input_data_folder_path):\n",
    "            input_data_path = input_data_folder_path+filename\n",
    "\n",
    "            with open(input_data_path, 'r', encoding='utf-8', errors='ignore') as file_in:\n",
    "                \n",
    "                for line in file_in:\n",
    "                    try:\n",
    "                        tweet = json.loads(line)\n",
    "                        tweet_obj = Tweet(tweet)\n",
    "                    except:\n",
    "                        print(tweet)\n",
    "                        continue\n",
    "                    \n",
    "                    if not tweet_obj.is_en():\n",
    "                        continue\n",
    "\n",
    "                    tweet_id = tweet_obj.get_id()\n",
    "                    user_id = tweet_obj.user.id_str\n",
    "                    feature_list = tweet_obj.get_tweet_features()\n",
    "                    file_out.write(tweet_id+'\\t'+user_id+'\\t')\n",
    "\n",
    "                    for f in feature_list:\n",
    "                        file_out.write(str(f))\n",
    "                        file_out.write('\\t')\n",
    "                    file_out.write('\\n')\n",
    "                    file_out.flush()\n",
    "                    \n",
    "    proc_date = proc_date+datetime.timedelta(days=1)\n",
    "    if proc_date == end_date_datetime:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c7810a-5333-4e74-82b5-fe6250ea17ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
